import { useState, useCallback, useRef } from 'react';
import { toast } from 'sonner';
import { supabase } from '@/integrations/supabase/client';

interface UseVoiceInputOptions {
  onTranscript?: (text: string) => void;
  onError?: (error: string) => void;
}

export function useVoiceInput({ onTranscript, onError }: UseVoiceInputOptions = {}) {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);

  const startLiveRecognition = useCallback((): Promise<string | null> => {
    return new Promise(async (resolve) => {
      try {
        // Request microphone access
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            sampleRate: 16000,
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          } 
        });
        
        streamRef.current = stream;
        setIsRecording(true);
        chunksRef.current = [];
        
        // Determine best supported format
        let mimeType = 'audio/webm';
        if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
          mimeType = 'audio/webm;codecs=opus';
        } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
          mimeType = 'audio/mp4';
        } else if (MediaRecorder.isTypeSupported('audio/ogg')) {
          mimeType = 'audio/ogg';
        }
        
        console.log('Using MIME type:', mimeType);
        
        const mediaRecorder = new MediaRecorder(stream, { mimeType });
        mediaRecorderRef.current = mediaRecorder;
        
        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) {
            chunksRef.current.push(e.data);
          }
        };
        
        mediaRecorder.onstop = async () => {
          // Stop all tracks
          stream.getTracks().forEach(track => track.stop());
          streamRef.current = null;
          
          if (chunksRef.current.length === 0) {
            setIsRecording(false);
            setIsProcessing(false);
            toast.error('Ù„Ù… ÙŠØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø£ÙŠ ØµÙˆØª');
            resolve(null);
            return;
          }
          
          setIsProcessing(true);
          toast.info('Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...');
          
          try {
            // Create audio blob
            const audioBlob = new Blob(chunksRef.current, { type: mimeType.split(';')[0] });
            console.log('Audio blob size:', audioBlob.size);
            
            // Convert to base64
            const reader = new FileReader();
            reader.onload = async () => {
              try {
                const base64 = (reader.result as string).split(',')[1];
                
                // Send to edge function
                const { data, error } = await supabase.functions.invoke('voice-to-text', {
                  body: { 
                    audio: base64,
                    mimeType: mimeType.split(';')[0]
                  }
                });
                
                setIsProcessing(false);
                setIsRecording(false);
                
                if (error) {
                  console.error('Voice-to-text error:', error);
                  toast.error('ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.');
                  onError?.('ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª');
                  resolve(null);
                  return;
                }
                
                if (data?.text) {
                  console.log('Transcription:', data.text);
                  toast.success('ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª Ø¨Ù†Ø¬Ø§Ø­');
                  onTranscript?.(data.text);
                  resolve(data.text);
                } else {
                  toast.error('Ù„Ù… ÙŠØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ù…. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.');
                  resolve(null);
                }
              } catch (err) {
                console.error('Error processing audio:', err);
                setIsProcessing(false);
                setIsRecording(false);
                toast.error('Ø­Ø¯Ø« Ø®Ø·Ø£. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.');
                resolve(null);
              }
            };
            
            reader.onerror = () => {
              setIsProcessing(false);
              setIsRecording(false);
              toast.error('ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ');
              resolve(null);
            };
            
            reader.readAsDataURL(audioBlob);
            
          } catch (error) {
            console.error('Error processing recording:', error);
            setIsProcessing(false);
            setIsRecording(false);
            toast.error('ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ³Ø¬ÙŠÙ„');
            resolve(null);
          }
        };
        
        mediaRecorder.start(100); // Collect data every 100ms
        toast.info('ðŸŽ™ï¸ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„... ØªØ­Ø¯Ø« Ø§Ù„Ø¢Ù†ØŒ Ø«Ù… Ø§Ø¶ØºØ· Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù', { duration: 3000 });
        
        // Auto-stop after 30 seconds
        setTimeout(() => {
          if (mediaRecorderRef.current?.state === 'recording') {
            toast.info('ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ (30 Ø«Ø§Ù†ÙŠØ© ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)');
            mediaRecorderRef.current.stop();
          }
        }, 30000);
        
      } catch (error) {
        console.error('Error starting recording:', error);
        setIsRecording(false);
        setIsProcessing(false);
        toast.error('ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¹Ø·Ø§Ø¡ Ø§Ù„Ø¥Ø°Ù†.');
        onError?.('ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†');
        resolve(null);
      }
    });
  }, [onTranscript, onError]);

  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current?.state === 'recording') {
      mediaRecorderRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
  }, []);

  return {
    isRecording,
    isProcessing,
    isSupported: true,
    startLiveRecognition,
    stopRecording,
  };
}
